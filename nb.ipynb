{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "\n",
    "from model import DCGAN\n",
    "from utils import pp, visualize, to_json, show_all_variables\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 250, \"Epoch to train [25]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_integer(\"train_size\", np.inf, \"The size of train images [np.inf]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"The size of batch images [64]\")\n",
    "flags.DEFINE_integer(\"input_height\", 160, \"The size of image to use (will be center cropped). [108]\")\n",
    "flags.DEFINE_integer(\"input_width\", None, \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n",
    "flags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\n",
    "flags.DEFINE_integer(\"output_width\", None, \"The size of the output images to produce. If None, same value as output_height [None]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"data\", \"Directory with image datasets [data]\")\n",
    "flags.DEFINE_string(\"dataset\", \"customData3\", \"The name of dataset [celebA, mnist, lsun]\")\n",
    "flags.DEFINE_string(\"input_fname_labels\", \"labels.txt\", \"The mapping between images and identities [*]\")\n",
    "flags.DEFINE_string(\"input_fname_pattern\", \"*.png\", \"Glob pattern of filename of input images [*]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_boolean(\"train\", True, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"crop\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\n",
    "flags.DEFINE_integer(\"generate_test_images\", 100, \"Number of images to generate during test. [100]\")\n",
    "flags.DEFINE_boolean(\"conditional\", True, \"Model and train conditional GAN\")\n",
    "flags.DEFINE_integer(\"loss_type\", 0, \"Loss type [0=cross entropy] 1=logloss 2=wasserstein\")\n",
    "flags.DEFINE_boolean(\"generate\", False, \"Generate 100 sample images for testing. Defaults to [False]\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'beta1': 0.5,\n",
      " 'checkpoint_dir': 'checkpoint',\n",
      " 'conditional': True,\n",
      " 'crop': False,\n",
      " 'data_dir': 'data',\n",
      " 'dataset': 'customData3',\n",
      " 'epoch': 250,\n",
      " 'generate': False,\n",
      " 'generate_test_images': 100,\n",
      " 'input_fname_labels': 'labels.txt',\n",
      " 'input_fname_pattern': '*.png',\n",
      " 'input_height': 160,\n",
      " 'input_width': None,\n",
      " 'learning_rate': 0.0002,\n",
      " 'loss_type': 0,\n",
      " 'output_height': 64,\n",
      " 'output_width': None,\n",
      " 'sample_dir': 'samples',\n",
      " 'train': True,\n",
      " 'train_size': inf,\n",
      " 'visualize': False}\n",
      "data/customData3/labels.txt\n",
      "(160, 160, 3)\n",
      "images.length: 2777, c_dim: 3, labels.length: 2777\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/g_h0_lin/Matrix:0 (float32_ref 110x1024) [112640, bytes: 450560]\n",
      "generator/g_h0_lin/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_bn0/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_bn0/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_h1_lin/Matrix:0 (float32_ref 1034x32768) [33882112, bytes: 135528448]\n",
      "generator/g_h1_lin/bias:0 (float32_ref 32768) [32768, bytes: 131072]\n",
      "generator/g_bn1/beta:0 (float32_ref 32768) [32768, bytes: 131072]\n",
      "generator/g_bn1/gamma:0 (float32_ref 32768) [32768, bytes: 131072]\n",
      "generator/g_h2/w:0 (float32_ref 5x5x128x138) [441600, bytes: 1766400]\n",
      "generator/g_h2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_h3/w:0 (float32_ref 5x5x3x138) [10350, bytes: 41400]\n",
      "generator/g_h3/biases:0 (float32_ref 3) [3, bytes: 12]\n",
      "discriminator/d_h0_conv/w:0 (float32_ref 5x5x13x13) [4225, bytes: 16900]\n",
      "discriminator/d_h0_conv/biases:0 (float32_ref 13) [13, bytes: 52]\n",
      "discriminator/d_h1_conv/w:0 (float32_ref 5x5x23x74) [42550, bytes: 170200]\n",
      "discriminator/d_h1_conv/biases:0 (float32_ref 74) [74, bytes: 296]\n",
      "discriminator/d_bn1/beta:0 (float32_ref 74) [74, bytes: 296]\n",
      "discriminator/d_bn1/gamma:0 (float32_ref 74) [74, bytes: 296]\n",
      "discriminator/d_h2_lin/Matrix:0 (float32_ref 18954x1024) [19408896, bytes: 77635584]\n",
      "discriminator/d_h2_lin/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_bn2/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_bn2/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_h3_lin/Matrix:0 (float32_ref 1034x1) [1034, bytes: 4136]\n",
      "discriminator/d_h3_lin/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 54008478\n",
      "Total bytes of variables: 216033912\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed\n",
      "Epoch: [ 0] [   0/  21] time: 13.6571, d_loss: 2.94851637, g_loss: 1.42200530\n",
      "Epoch: [ 0] [   1/  21] time: 26.3082, d_loss: 1.74418259, g_loss: 0.92199582\n",
      "Epoch: [ 0] [   2/  21] time: 38.8868, d_loss: 1.50230038, g_loss: 0.81870896\n",
      "Epoch: [ 0] [   3/  21] time: 51.3087, d_loss: 1.51728117, g_loss: 0.81185359\n",
      "Epoch: [ 0] [   4/  21] time: 63.7380, d_loss: 1.37586832, g_loss: 0.74292755\n",
      "Epoch: [ 0] [   5/  21] time: 76.1355, d_loss: 1.38962317, g_loss: 0.76314420\n",
      "Epoch: [ 0] [   6/  21] time: 88.4958, d_loss: 1.36279416, g_loss: 0.75710583\n",
      "Epoch: [ 0] [   7/  21] time: 100.9024, d_loss: 1.32421041, g_loss: 0.76833302\n",
      "Epoch: [ 0] [   8/  21] time: 113.2475, d_loss: 1.29551864, g_loss: 0.75604725\n",
      "Epoch: [ 0] [   9/  21] time: 125.5418, d_loss: 1.28095508, g_loss: 0.76271188\n",
      "Epoch: [ 0] [  10/  21] time: 137.8122, d_loss: 1.25165057, g_loss: 0.78731608\n",
      "Epoch: [ 0] [  11/  21] time: 150.0867, d_loss: 1.23899412, g_loss: 0.80553591\n",
      "Epoch: [ 0] [  12/  21] time: 162.4278, d_loss: 1.23073900, g_loss: 0.83160579\n",
      "Epoch: [ 0] [  13/  21] time: 174.7610, d_loss: 1.21202326, g_loss: 0.85573268\n",
      "Epoch: [ 0] [  14/  21] time: 187.1334, d_loss: 1.21741486, g_loss: 0.88760328\n",
      "Epoch: [ 0] [  15/  21] time: 199.4557, d_loss: 1.24552202, g_loss: 0.91347778\n",
      "Epoch: [ 0] [  16/  21] time: 211.7178, d_loss: 1.23170733, g_loss: 0.90185893\n",
      "Epoch: [ 0] [  17/  21] time: 224.1064, d_loss: 1.23066592, g_loss: 0.89173907\n",
      "Epoch: [ 0] [  18/  21] time: 236.8334, d_loss: 1.20330369, g_loss: 0.84067965\n",
      "Epoch: [ 0] [  19/  21] time: 249.2501, d_loss: 1.23055911, g_loss: 0.84255236\n",
      "Epoch: [ 0] [  20/  21] time: 261.8418, d_loss: 1.17747939, g_loss: 0.86179656\n",
      "Epoch: [ 1] [   0/  21] time: 274.2968, d_loss: 1.12660193, g_loss: 0.89288747\n",
      "Epoch: [ 1] [   1/  21] time: 286.7202, d_loss: 1.10419202, g_loss: 0.92266130\n",
      "Epoch: [ 1] [   2/  21] time: 299.6288, d_loss: 1.07055879, g_loss: 0.92717493\n",
      "Epoch: [ 1] [   3/  21] time: 312.3723, d_loss: 1.04483390, g_loss: 0.95648837\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    if FLAGS.input_width is None:\n",
    "        FLAGS.input_width = FLAGS.input_height\n",
    "    if FLAGS.output_width is None:\n",
    "        FLAGS.output_width = FLAGS.output_height\n",
    "\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    if not os.path.exists(FLAGS.sample_dir):\n",
    "        os.makedirs(FLAGS.sample_dir)\n",
    "\n",
    "    # y_dim is inferred from the dataset name or the labels file\n",
    "    if FLAGS.conditional and FLAGS.dataset != 'mnist':\n",
    "        labels_fname = os.path.join(FLAGS.data_dir, FLAGS.dataset, FLAGS.input_fname_labels)\n",
    "        print(labels_fname)\n",
    "        if not os.path.exists(labels_fname):\n",
    "            raise Exception(\"[!] conditional requires image<->identity labels\")\n",
    "\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    run_config = tf.ConfigProto()\n",
    "    run_config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=run_config) as sess:\n",
    "        dcgan = DCGAN(\n",
    "            sess,\n",
    "            input_width=FLAGS.input_width,\n",
    "            input_height=FLAGS.input_height,\n",
    "            output_width=FLAGS.output_width,\n",
    "            output_height=FLAGS.output_height,\n",
    "            batch_size=FLAGS.batch_size,\n",
    "            sample_num=FLAGS.batch_size,\n",
    "            z_dim=FLAGS.generate_test_images,\n",
    "            data_dir=FLAGS.data_dir,\n",
    "            dataset_name=FLAGS.dataset,\n",
    "            input_fname_pattern=FLAGS.input_fname_pattern,\n",
    "            crop=FLAGS.crop,\n",
    "            checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "            sample_dir=FLAGS.sample_dir,\n",
    "            conditional = FLAGS.conditional,\n",
    "            loss_type=FLAGS.loss_type)\n",
    "\n",
    "        show_all_variables()\n",
    "\n",
    "        if FLAGS.train:\n",
    "            dcgan.train(FLAGS)\n",
    "        else:\n",
    "            if not dcgan.load(FLAGS.checkpoint_dir)[0]:\n",
    "                raise Exception(\"[!] Train a model first, then run test mode\")\n",
    "\n",
    "            if FLAGS.generate:\n",
    "                generate_samples(sess, dcgan, FLAGS)\n",
    "            exit()\n",
    "        # to_json(\"./web/js/layers.js\", [dcgan.h0_w, dcgan.h0_b, dcgan.g_bn0],\n",
    "        #                 [dcgan.h1_w, dcgan.h1_b, dcgan.g_bn1],\n",
    "        #                 [dcgan.h2_w, dcgan.h2_b, dcgan.g_bn2],\n",
    "        #                 [dcgan.h3_w, dcgan.h3_b, dcgan.g_bn3],\n",
    "        #                 [dcgan.h4_w, dcgan.h4_b, None])\n",
    "\n",
    "        # Below is codes for visualization\n",
    "        OPTION = 1\n",
    "        visualize(sess, dcgan, FLAGS, OPTION)\n",
    "     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
